---
output: html_document
---

# Automatic analysis of exercise quality
### Machine learning, course project

---------------------------------------------------------------------

## Synopsis

### The problem

The data used for this project has 
been downloaded here:

http://groupware.les.inf.puc-rio.br/har

Six young health participants were 
asked to perform one set of 10 repetitions 
of the Unilateral Dumbbell Biceps Curl in five different fashions -
the correct one (Class A) and with a four typical mistakes
(Classes B, C, D, E). The goal of our analysis
is to construct a prediction algorithm to
automatically determine the class using
other 159 variables as predictors.

### Summary of analysis

We first looked at the given data and identified
about 50 variables that were actually measured (the
rest are either missing values or irrelevant things like
the time of measurement).
Then we constructed a simple decision tree several times
based on a random subsample to identify the only 14 variables
that affect the outcome.

Further, we split our data into a training set proper
and a validation set, removed all the variables but 
the important 14 from the data
and constructed a model based on boosting with
trees. Its accuracy is 0.93. We used it to correctly 
guess 18 out of 20 test examplels.

Finally, we create a random forest model
with accuracy about 0.99 and used it to correctly
guess the remaining 2 test examples.

## Exploratory analysis and cleaning the data.

Downloading etc.:

```{r cache=TRUE, echo=TRUE, message=TRUE}
library(ggplot2)
library(knitr)
library(rpart)
library(caret)
library(lattice)
library(rpart.plot)
library(rattle)
library(plyr)
library(survival)

if (!file.exists("pml-training.csv")) { 
        url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(url,"pml-training.csv", method="curl")       
}

if (!file.exists("pml-testing.csv")) { 
        url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(url,"pml-testing.csv", method="curl")       
}

D <- read.csv("pml-training.csv")
T <- read.csv("pml-testing.csv")
set.seed(2603)
```

The training set contains `r nrow(D)` rows
and the test set contains `r nrow(T)` rows.

Now we'll remove the first 7 variables because they identify the subject,
the time etc. - things not relevant to prediction).
```{r}
training <- D[,8:160]
testing <- T[,8:160]
types <- data.frame(var=names(testing),type=as.character(lapply(testing,class)))
head(types)
head(testing[,1:8])
```

As we see, a lot of variables do not actually have any values.
As it happens, only relevant variables are those numberic ones.
We'll re-define the training and the testing sets:
```{r}
relevant_var <- (types$type=='numeric')|(types$type=='integer')
training <- training[,relevant_var]
training$classe <- D$classe
testing <- testing[,relevant_var]
dim(training)
dim(testing)
```

## Decision tree that gives us important variables

Since the variable we are predicting is categorical,
we won't even look at algorithms related to linear models
and build our analysis entirely on trees. We'll construct 
a simple decision tree first to see which variables
are imporant for prediction.

```{r cache=TRUE}
modTree <- train(classe ~ ., method="rpart", data=training)
fancyRpartPlot(modTree$finalModel)
var.imp <- varImp(modTree)
print(var.imp)
```

I tried several times and the set of 14 important variables is
always the same, so we'll use these 14 in to create our model.
Here is a dot plot
of the two most important variables
colored by the class.

```{r cache=TRUE}
good.var <- row.names(var.imp$importance)[1:14]
qplot(data=training, x=magnet_dumbbell_y, y=pitch_forearm, colour=classe)
```

However, any particular decision tree itself has a very low accuracy.
We'll try a boosting algorithm based 

## The boosting model.

### Creating the model

We'll use a 'GBM' model because it is also based on trees
so it seems reasonable to combine it with the variables
that we got from obtaining a simple decision tree.

First, we'll remove all the variables
but the relevant 14 and create a training subset
and a validation set to be able to estimate the accuracy
of our algorithm later.

```{r cache=TRUE}
inTrain <- createDataPartition(y=training$classe,
                               p=0.7, list=FALSE)
training.set <- training[inTrain,]
validation.set <- training[-inTrain,]
dim(training.set)
dim(validation.set)
training.set <- training.set[,c(good.var,"classe")]
validation.set <- validation.set[,c(good.var,"classe")]
testing <- testing[,c(good.var,"problem_id")]
```

We'll predict the class on the validation set and
compare with the known answer:

```{r cache=TRUE, results="hide"}
modGBM <- train(classe ~ ., method="gbm",data=training.set)
print(modGBM)

predictGBM <- predict(modGBM, newdata=validation.set)
C <- confusionMatrix(predictGBM,validation.set$classe)
```

Here is the whole confusion matrix
```{r cache=TRUE}
C$table
```

The overall accuracy is `r C$overall[1]`.
The actual accuracy on the given test set is 18 out of 20, i.e.,
0.95.

## Random forest

We also constructed a random forest model
with 0.99 accuracy and used it to correct the two answers
that were guessed wrongly by the GBM algorithm.
However, it takes a long time to actually construct it
(20 minutes or so on my computer),
so I won't include it ito my analysis.

### System info

Our analysis has been originally created and run
in RStudio v. 0.98.1080 under OS X 10.9.5.

Time and date the report has been generated:
`r Sys.time()` (Central European time).